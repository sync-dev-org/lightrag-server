###########################
### Server Configuration
###########################
HOST=0.0.0.0
PORT=9621
WEBUI_TITLE='Lightrag Server'
WEBUI_DESCRIPTION="Simple and Fast Graph Based RAG System"
WORKERS=4

### gunicorn worker timeout(as default LLM request timeout if LLM_TIMEOUT is not set)
# TIMEOUT=150
# CORS_ORIGINS=http://localhost:3000,http://localhost:8080

### Optional SSL Configuration
# SSL=true
# SSL_CERTFILE=/path/to/cert.pem
# SSL_KEYFILE=/path/to/key.pem

#####################################
### Login and API-Key Configuration
#####################################
# AUTH_ACCOUNTS='admin:admin123,user1:pass456'
# TOKEN_SECRET=Your-Key-For-LightRAG-API-Server
# TOKEN_EXPIRE_HOURS=48
# GUEST_TOKEN_EXPIRE_HOURS=24
# JWT_ALGORITHM=HS256

### API-Key to access LightRAG Server API
# LIGHTRAG_API_KEY=your-secure-api-key-here
# WHITELIST_PATHS=/health,/api/*

######################################################################################
### Query Configuration
###
### How to control the context lenght sent to LLM:
###    MAX_ENTITY_TOKENS + MAX_RELATION_TOKENS < MAX_TOTAL_TOKENS
###    Chunk_Tokens = MAX_TOTAL_TOKENS - Actual_Entity_Tokens - Actual_Reation_Tokens
######################################################################################
# LLM responde cache for query (Not valid for streaming response)
ENABLE_LLM_CACHE=true

#########################################################
### Reranking configuration
### RERANK_BINDING type:  null, cohere, jina, aliyun
### For rerank model deployed by vLLM use cohere binding
#########################################################
RERANK_BINDING=cohere

RERANK_MODEL=rerank-v3.5
RERANK_BINDING_HOST=http://127.0.0.1:${LITELLM_PORT:-4000}/rerank
# RERANK_BINDING_API_KEY=YOUR_API_KEY_HERE


########################################
### Document processing configuration
########################################
ENABLE_LLM_CACHE_FOR_EXTRACT=true

### Document processing output language: English, Chinese, French, German ...
SUMMARY_LANGUAGE=English

### Entity types that the LLM will attempt to recognize
# ENTITY_TYPES='["Person", "Creature", "Organization", "Location", "Event", "Concept", "Method", "Content", "Data", "Artifact", "NaturalObject"]'

### Chunk size for document splitting, 500~1500 is recommended
CHUNK_SIZE=1500
CHUNK_OVERLAP_SIZE=200

###############################
### Concurrency Configuration
###############################
### Max concurrency requests of LLM (for both query and document processing)
MAX_ASYNC=8
### Number of parallel processing documents(between 2~10, MAX_ASYNC/3 is recommended)
MAX_PARALLEL_INSERT=4
### Max concurrency requests for Embedding
EMBEDDING_FUNC_MAX_ASYNC=8
### Num of chunks send to Embedding in single request
EMBEDDING_BATCH_NUM=10

###########################################################
### LLM Configuration
### LLM_BINDING type: openai, ollama, lollms, azure_openai, aws_bedrock
###########################################################
### LLM request timeout setting for all llm (0 means no timeout for Ollma)
LLM_TIMEOUT=180

LLM_BINDING=openai
LLM_BINDING_HOST="http://127.0.0.1:${LITELLM_PORT:-4000}"
LLM_MODEL=gpt-oss-120b-cerebras
# LLM_BINDING_API_KEY=YOUR_API_KEY_HERE

EMBEDDING_BINDING=openai
EMBEDDING_BINDING_HOST="http://127.0.0.1:${LITELLM_PORT:-4000}"
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIM=1536
# EMBEDDING_BINDING_API_KEY=YOUR_API_KEY_HERE


####################################################################
### WORKSPACE setting workspace name for all storage types
### in the purpose of isolating data from LightRAG instances.
### Valid workspace name constraints: a-z, A-Z, 0-9, and _
####################################################################
# WORKSPACE=${WORKSPACE_NAME:-space1}

############################
### Data storage selection
############################
LIGHTRAG_KV_STORAGE=JsonKVStorage
LIGHTRAG_DOC_STATUS_STORAGE=JsonDocStatusStorage
LIGHTRAG_GRAPH_STORAGE=NetworkXStorage
LIGHTRAG_VECTOR_STORAGE=FaissVectorDBStorage
